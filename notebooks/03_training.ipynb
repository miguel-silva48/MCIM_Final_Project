{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Image Captioning - Model Training\n",
    "\n",
    "This notebook trains an encoder-decoder model with attention mechanism for generating medical image captions from chest X-rays.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "- **Encoder**: DenseNet-121 pretrained on ImageNet, fine-tuned for medical images\n",
    "- **Decoder**: LSTM with Bahdanau (additive) attention mechanism\n",
    "- **Vocabulary**: 514 tokens from Indiana University Chest X-Ray dataset\n",
    "\n",
    "## Training Objectives\n",
    "\n",
    "1. **Minimize Cross-Entropy Loss**: Learn to predict correct word sequences\n",
    "2. **Maximize BLEU/METEOR/ROUGE-L**: Generate captions similar to radiologist reports\n",
    "3. **Learn Meaningful Attention**: Focus on relevant anatomical regions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/marialinhares/uni/masters/1year1semester/MCIM/MCIM_Final_Project\n",
      "Working directory: /Users/marialinhares/uni/masters/1year1semester/MCIM/MCIM_Final_Project/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Check execution environment\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os, yaml, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for imports\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import project modules\n",
    "from src.models.caption_model import EncoderDecoderModel\n",
    "from src.data.dataset import ChestXrayDataset\n",
    "from src.data.vocabulary import Vocabulary\n",
    "from src.data.transforms import get_transforms\n",
    "from src.data.collate import collate_fn\n",
    "from src.training.trainer import CaptionTrainer\n",
    "from src.training.metrics import CaptionMetrics\n",
    "from src.utils.device_check import get_device, get_device_info, print_device_info\n",
    "\n",
    "print(\"✓ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization settings configured\n"
     ]
    }
   ],
   "source": [
    "# Matplotlib configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"✓ Visualization settings configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "The project uses YAML configuration files for all hyperparameters:\n",
    "\n",
    "- **model_config.yaml**: Model architecture, training hyperparameters, inference settings\n",
    "- **data_config.yaml**: Data processing, splits, vocabulary settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration files loaded\n"
     ]
    }
   ],
   "source": [
    "# Load configurations\n",
    "config_path = project_root / 'configs' / 'model_config.yaml'\n",
    "data_config_path = project_root / 'configs' / 'data_config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "with open(data_config_path, 'r') as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✓ Configuration files loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: MPS (Apple Silicon)\n",
      "\n",
      "Training Configuration:\n",
      "============================================================\n",
      "  Epochs: 30\n",
      "  Batch Size: 32\n",
      "  Learning Rate: 0.0001\n",
      "  Gradient Clip: 5.0\n",
      "  Early Stopping Patience: 10\n",
      "  Inference Method: beam_search\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Device detection\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"Using device: CUDA - {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"Using device: MPS (Apple Silicon)\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"Using device: CPU\")\n",
    "\n",
    "# Display key hyperparameters\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['training']['optimizer']['learning_rate']}\")\n",
    "print(f\"  Gradient Clip: {config['training']['gradient_clip_norm']}\")\n",
    "print(f\"  Early Stopping Patience: {config['training']['early_stopping']['patience']}\")\n",
    "print(f\"  Inference Method: {config['inference']['decoding']['method']}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "The data pipeline follows this flow:\n",
    "\n",
    "1. **Vocabulary**: Load 514 tokens built from training data only (prevents leakage)\n",
    "2. **Transforms**: \n",
    "   - Train: Random rotation, color jitter (NO horizontal flip to preserve anatomical laterality)\n",
    "   - Val/Test: Center crop and normalization only\n",
    "3. **Dataset**: PyTorch Dataset with proper padding and special tokens\n",
    "4. **DataLoader**: Custom collate function sorts captions by length for efficient packing\n",
    "\n",
    "**Important**: Patient-level splits ensure no data leakage between train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vocabulary loaded: 514 tokens\n",
      "  Special tokens: PAD=0, START=1, END=2, UNK=3\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "processed_dir = project_root / 'data' / 'processed' / 'first_frontal_impression'\n",
    "image_dir = project_root / 'data' / 'images' / 'images_normalized'\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = Vocabulary(str(processed_dir / 'vocabulary.txt'))\n",
    "print(f\"✓ Vocabulary loaded: {len(vocab)} tokens\")\n",
    "print(f\"  Special tokens: PAD={vocab.PAD_IDX}, START={vocab.START_IDX}, END={vocab.END_IDX}, UNK={vocab.UNK_IDX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Image transforms created\n",
      "  Train: RandomRotation + ColorJitter + Normalize\n",
      "  Val/Test: CenterCrop + Normalize\n"
     ]
    }
   ],
   "source": [
    "# Create transforms\n",
    "train_transform = get_transforms(mode='train', image_size=224)\n",
    "val_transform = get_transforms(mode='val', image_size=224)\n",
    "\n",
    "print(\"✓ Image transforms created\")\n",
    "print(\"  Train: RandomRotation + ColorJitter + Normalize\")\n",
    "print(\"  Val/Test: CenterCrop + Normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset sizes:\n",
      "  Train: 2948 samples\n",
      "  Val:   368 samples\n",
      "  Test:  370 samples\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = ChestXrayDataset(\n",
    "    csv_file=str(processed_dir / 'train.csv'),\n",
    "    image_dir=str(image_dir),\n",
    "    vocabulary=vocab,\n",
    "    transform=train_transform,\n",
    "    max_caption_length=config['data']['max_caption_length']\n",
    ")\n",
    "\n",
    "val_dataset = ChestXrayDataset(\n",
    "    csv_file=str(processed_dir / 'val.csv'),\n",
    "    image_dir=str(image_dir),\n",
    "    vocabulary=vocab,\n",
    "    transform=val_transform,\n",
    "    max_caption_length=config['data']['max_caption_length']\n",
    ")\n",
    "\n",
    "test_dataset = ChestXrayDataset(\n",
    "    csv_file=str(processed_dir / 'test.csv'),\n",
    "    image_dir=str(image_dir),\n",
    "    vocabulary=vocab,\n",
    "    transform=val_transform,\n",
    "    max_caption_length=config['data']['max_caption_length']\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val:   {len(val_dataset)} samples\")\n",
    "print(f\"  Test:  {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaders created\n",
      "  Batch size: 32\n",
      "  Batches per epoch: Train=93, Val=12, Test=12\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = config['training']['batch_size']\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=config['data']['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,  # Disable for test (cleaner output)\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"✓ Data loaders created\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Batches per epoch: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = EncoderDecoderModel(\n",
    "    vocab_size=len(vocab),\n",
    "    encoder_architecture=config['model']['encoder']['architecture'],\n",
    "    encoder_pretrained=config['model']['encoder']['pretrained'],\n",
    "    encoder_freeze=config['model']['encoder']['freeze_backbone'],\n",
    "    encoder_feature_dim=config['model']['encoder']['output_feature_dim'],\n",
    "    attention_type=config['model']['decoder']['attention']['type'],\n",
    "    attention_dim=config['model']['decoder']['attention']['attention_dim'],\n",
    "    embedding_dim=config['model']['decoder']['embedding_dim'],\n",
    "    decoder_dim=config['model']['decoder']['hidden_dim'],\n",
    "    num_decoder_layers=config['model']['decoder']['num_layers'],\n",
    "    dropout=config['model']['decoder']['dropout'],\n",
    "    pad_idx=vocab.PAD_IDX,\n",
    "    start_idx=vocab.START_IDX,\n",
    "    end_idx=vocab.END_IDX,\n",
    "    unk_idx=vocab.UNK_IDX\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"✓ Model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:\n",
      "============================================================\n",
      "  Total Parameters: 22,412,139\n",
      "  Trainable Parameters: 15,458,283\n",
      "  Frozen Parameters: 6,953,856\n",
      "  Memory: ~85.5 MB (FP32)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Total Parameters: {total_params:,}\")\n",
    "print(f\"  Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"  Frozen Parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"  Memory: ~{total_params * 4 / (1024**2):.1f} MB (FP32)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing forward pass...\n",
      "✓ Forward pass successful!\n",
      "  Input images: torch.Size([32, 3, 224, 224])\n",
      "  Predictions: torch.Size([32, 48, 514])\n",
      "  Attention weights shape: torch.Size([32, 48, 49])\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "sample_batch = next(iter(train_loader))\n",
    "images, captions, caption_lengths, _, _ = sample_batch\n",
    "images = images.to(device)\n",
    "captions = captions.to(device)\n",
    "caption_lengths = caption_lengths.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions, attention_weights, sorted_captions, sorted_lengths = model(\n",
    "        images, captions[:, :-1], caption_lengths - 1\n",
    "    )\n",
    "\n",
    "print(f\"✓ Forward pass successful!\")\n",
    "print(f\"  Input images: {images.shape}\")\n",
    "print(f\"  Predictions: {predictions.shape}\")\n",
    "print(f\"  Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Execution\n",
    "\n",
    "The **CaptionTrainer** handles the complete training loop:\n",
    "\n",
    "### Training Loop (per epoch):\n",
    "1. **Forward pass**: Images → Encoder → Attention → Decoder → Predictions\n",
    "2. **Loss calculation**: Cross-entropy with label smoothing + perplexity\n",
    "3. **Backpropagation**: Gradient clipping to prevent exploding gradients\n",
    "4. **Validation**: Generate captions with beam search, compute BLEU/METEOR/ROUGE-L\n",
    "5. **Checkpointing**: Save best model based on validation BLEU-4\n",
    "6. **Early stopping**: Stop if validation metrics don't improve for N epochs\n",
    "\n",
    "### Features:\n",
    "- Mixed precision training (FP16) for faster training on compatible GPUs\n",
    "- Learning rate scheduling (ReduceLROnPlateau)\n",
    "- Progress bars with tqdm\n",
    "- Comprehensive logging (metrics CSV, training manifest JSON)\n",
    "\n",
    "**Note**: Training can take 2-4 hours on GPU, 8-12 hours on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output directory: /Users/marialinhares/uni/masters/1year1semester/MCIM/MCIM_Final_Project/outputs/training_runs/densenet121_bahdanau_20251230_224113\n"
     ]
    }
   ],
   "source": [
    "# Create output directory with timestamp\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "variant_name = f\"{config['model']['encoder']['architecture']}_{config['model']['decoder']['attention']['type']}\"\n",
    "output_dir = project_root / 'outputs' / 'training_runs' / f\"{variant_name}_{timestamp}\"\n",
    "\n",
    "print(f\"Training output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer initialized\n"
     ]
    }
   ],
   "source": [
    "# Create trainer\n",
    "trainer = CaptionTrainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    vocabulary=vocab,\n",
    "    config=config,\n",
    "    device=device,\n",
    "    output_dir=str(output_dir),\n",
    "    resume_checkpoint=None  # Set path to resume training\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "Starting training for 30 epochs\n",
      "Output directory: /Users/marialinhares/uni/masters/1year1semester/MCIM/MCIM_Final_Project/outputs/training_runs/densenet121_bahdanau_20251230_224113/first_frontal_impression_20251230_224113\n",
      "Device: mps\n",
      "Mixed precision: False\n",
      "Batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/93 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1056) to match target batch_size (1536).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Best model saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbest_model.pt\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/src/training/trainer.py:164\u001b[39m, in \u001b[36mCaptionTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    161\u001b[39m epoch_start_time = time.time()\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m train_metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    167\u001b[39m val_metrics = \u001b[38;5;28mself\u001b[39m._validate_epoch(epoch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/src/training/trainer.py:255\u001b[39m, in \u001b[36mCaptionTrainer._train_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    249\u001b[39m predictions, attention_weights, sorted_captions, sorted_lengths = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    250\u001b[39m     images, captions[:, :-\u001b[32m1\u001b[39m], caption_lengths - \u001b[32m1\u001b[39m\n\u001b[32m    251\u001b[39m )\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# Calculate loss (use sorted_captions from model output)\u001b[39;00m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# targets: sorted_captions[:, 1:] (skip <START>, include <END>)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_captions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# Calculate perplexity (use sorted_captions from model output)\u001b[39;00m\n\u001b[32m    258\u001b[39m perplexity = \u001b[38;5;28mself\u001b[39m.perplexity_fn(predictions, sorted_captions[:, \u001b[32m1\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/src/training/loss.py:70\u001b[39m, in \u001b[36mCaptionLoss.forward\u001b[39m\u001b[34m(self, predictions, targets, caption_lengths)\u001b[39m\n\u001b[32m     67\u001b[39m targets = targets.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni/masters/1year1semester/MCIM/MCIM_Final_Project/venv/lib/python3.12/site-packages/torch/nn/functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (1056) to match target batch_size (1536)."
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"✓ Best model saved to: {output_dir / 'checkpoints' / 'best_model.pt'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Visualization\n",
    "\n",
    "Analyze training dynamics through loss curves and validation metrics:\n",
    "\n",
    "- **Loss & Perplexity**: Should decrease over epochs (lower is better)\n",
    "- **BLEU Scores**: Should increase over epochs (higher is better, max 1.0)\n",
    "- **METEOR & ROUGE-L**: Alternative metrics that consider synonyms and word order\n",
    "\n",
    "### Expected Behavior:\n",
    "- Training loss decreases smoothly\n",
    "- Validation loss follows training loss (no severe overfitting)\n",
    "- BLEU-4 typically reaches 0.15-0.30 for medical image captioning\n",
    "- Learning rate decreases when validation metrics plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "metrics_df = pd.read_csv(output_dir / 'metrics.csv')\n",
    "\n",
    "print(f\"Training Metrics Loaded:\")\n",
    "print(f\"  Total epochs trained: {len(metrics_df)}\")\n",
    "print(f\"\\nFirst 5 epochs:\")\n",
    "display(metrics_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Train/Val Loss\n",
    "axes[0, 0].plot(metrics_df['epoch'], metrics_df['train_loss'], 'o-', label='Train', linewidth=2)\n",
    "axes[0, 0].plot(metrics_df['epoch'], metrics_df['val_loss'], 's-', label='Validation', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Perplexity\n",
    "axes[0, 1].plot(metrics_df['epoch'], metrics_df['train_perplexity'], 'o-', label='Train', linewidth=2)\n",
    "axes[0, 1].plot(metrics_df['epoch'], metrics_df['val_perplexity'], 's-', label='Validation', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Perplexity')\n",
    "axes[0, 1].set_title('Perplexity (lower is better)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# BLEU Scores\n",
    "axes[1, 0].plot(metrics_df['epoch'], metrics_df['val_bleu_4'], 'o-', label='BLEU-4', linewidth=2)\n",
    "axes[1, 0].plot(metrics_df['epoch'], metrics_df['val_bleu_3'], 's-', label='BLEU-3', linewidth=2)\n",
    "axes[1, 0].plot(metrics_df['epoch'], metrics_df['val_bleu_2'], '^-', label='BLEU-2', linewidth=2)\n",
    "axes[1, 0].plot(metrics_df['epoch'], metrics_df['val_bleu_1'], 'v-', label='BLEU-1', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('BLEU Score')\n",
    "axes[1, 0].set_title('BLEU Scores on Validation Set')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# METEOR & ROUGE-L\n",
    "axes[1, 1].plot(metrics_df['epoch'], metrics_df['val_meteor'], 'o-', label='METEOR', linewidth=2)\n",
    "axes[1, 1].plot(metrics_df['epoch'], metrics_df['val_rouge_l'], 's-', label='ROUGE-L', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('METEOR & ROUGE-L on Validation Set')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = output_dir / 'training_curves.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved to: {save_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best epoch\n",
    "best_epoch = metrics_df.loc[metrics_df['val_bleu_4'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest Epoch: {int(best_epoch['epoch'])}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Val Loss: {best_epoch['val_loss']:.4f}\")\n",
    "print(f\"  Val Perplexity: {best_epoch['val_perplexity']:.4f}\")\n",
    "print(f\"  BLEU-4: {best_epoch['val_bleu_4']:.4f}\")\n",
    "print(f\"  BLEU-3: {best_epoch['val_bleu_3']:.4f}\")\n",
    "print(f\"  BLEU-2: {best_epoch['val_bleu_2']:.4f}\")\n",
    "print(f\"  BLEU-1: {best_epoch['val_bleu_1']:.4f}\")\n",
    "print(f\"  METEOR: {best_epoch['val_meteor']:.4f}\")\n",
    "print(f\"  ROUGE-L: {best_epoch['val_rouge_l']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Set Evaluation\n",
    "\n",
    "Evaluate the best model on the held-out test set for final performance assessment.\n",
    "\n",
    "### Evaluation Process:\n",
    "1. Load best checkpoint (highest validation BLEU-4)\n",
    "2. Generate captions for all test images using beam search\n",
    "3. Compute metrics: BLEU-1/2/3/4, METEOR, ROUGE-L\n",
    "4. Compare test vs validation performance\n",
    "\n",
    "### Expected Results:\n",
    "- Test metrics should be similar to validation metrics\n",
    "- Large gap indicates overfitting\n",
    "- BLEU-4 typically ranges from 0.15-0.30 for medical images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "from src.utils.checkpoint import CheckpointManager\n",
    "\n",
    "checkpoint_mgr = CheckpointManager(str(output_dir / 'checkpoints'))\n",
    "checkpoint_info = checkpoint_mgr.load_checkpoint(\n",
    "    str(output_dir / 'checkpoints' / 'best_model.pt'),\n",
    "    model=model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded best model from epoch {checkpoint_info['epoch']}\")\n",
    "print(f\"  Validation BLEU-4: {checkpoint_info.get('metrics', {}).get('val_bleu_4', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "metrics_calculator = CaptionMetrics()\n",
    "all_references, all_hypotheses = [], []\n",
    "test_samples = []\n",
    "\n",
    "print(\"\\nGenerating captions for test set...\")\n",
    "with torch.no_grad():\n",
    "    for images, captions, caption_lengths, image_paths, uids in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            # Generate caption\n",
    "            method = config['inference']['decoding']['method']\n",
    "            beam_size = 1 if method == 'greedy' else config['inference']['decoding']['beam_size']\n",
    "\n",
    "            generated_ids, attention_weights = model.generate_caption(\n",
    "                images[i:i+1],\n",
    "                max_length=config['inference']['decoding']['max_length'],\n",
    "                beam_size=beam_size,\n",
    "                length_penalty=config['inference']['decoding'].get('length_penalty', 0.0)\n",
    "            )\n",
    "\n",
    "            # Decode reference\n",
    "            reference_tokens = [\n",
    "                vocab.idx_to_token.get(idx, '<UNK>')\n",
    "                for idx in captions[i].cpu().tolist()\n",
    "                if idx not in [vocab.PAD_IDX, vocab.START_IDX]\n",
    "            ]\n",
    "            if vocab.END_TOKEN in reference_tokens:\n",
    "                reference_tokens = reference_tokens[:reference_tokens.index(vocab.END_TOKEN)]\n",
    "\n",
    "            # Decode hypothesis\n",
    "            hypothesis_tokens = [\n",
    "                vocab.idx_to_token.get(idx, '<UNK>')\n",
    "                for idx in generated_ids\n",
    "                if idx not in [vocab.PAD_IDX, vocab.START_IDX]\n",
    "            ]\n",
    "            if vocab.END_TOKEN in hypothesis_tokens:\n",
    "                hypothesis_tokens = hypothesis_tokens[:hypothesis_tokens.index(vocab.END_TOKEN)]\n",
    "\n",
    "            all_references.append([reference_tokens])\n",
    "            all_hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "            # Store samples for visualization\n",
    "            if len(test_samples) < 20:\n",
    "                test_samples.append({\n",
    "                    'uid': int(uids[i]),\n",
    "                    'image_path': image_paths[i],\n",
    "                    'reference': ' '.join(reference_tokens),\n",
    "                    'generated': ' '.join(hypothesis_tokens),\n",
    "                    'attention_weights': attention_weights\n",
    "                })\n",
    "\n",
    "print(f\"✓ Generated {len(all_hypotheses)} captions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics\n",
    "test_metrics = metrics_calculator.compute_all_metrics(all_references, all_hypotheses)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(\"=\"*60)\n",
    "for metric_name, value in test_metrics.items():\n",
    "    print(f\"  {metric_name.upper()}: {value:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare test vs validation\n",
    "val_metrics = {\n",
    "    'bleu_4': best_epoch['val_bleu_4'],\n",
    "    'bleu_3': best_epoch['val_bleu_3'],\n",
    "    'bleu_2': best_epoch['val_bleu_2'],\n",
    "    'bleu_1': best_epoch['val_bleu_1'],\n",
    "    'meteor': best_epoch['val_meteor'],\n",
    "    'rouge_l': best_epoch['val_rouge_l']\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "metrics_names = list(test_metrics.keys())\n",
    "test_values = [test_metrics[m] for m in metrics_names]\n",
    "val_values = [val_metrics[m] for m in metrics_names]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, val_values, width, label='Validation', alpha=0.8)\n",
    "ax.bar(x + width/2, test_values, width, label='Test', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Test vs Validation Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.upper() for m in metrics_names])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = output_dir / 'test_vs_val_metrics.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved to: {save_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sample Predictions\n",
    "\n",
    "Qualitative assessment of generated captions. Good captions should:\n",
    "- Mention key anatomical structures (heart, lungs, etc.)\n",
    "- Identify abnormalities if present (atelectasis, effusion, etc.)\n",
    "- Use appropriate medical terminology\n",
    "- Be concise and clear (similar to radiologist impressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample predictions\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for idx, sample in enumerate(test_samples[:10], 1):\n",
    "    print(f\"\\n[Sample {idx}]\")\n",
    "    print(f\"  Reference:  {sample['reference']}\")\n",
    "    print(f\"  Generated:  {sample['generated']}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention Visualization\n",
    "\n",
    "Visualize attention weights to understand where the model \"looks\" when generating each word.\n",
    "\n",
    "### Interpretation:\n",
    "- **Hot colors (red/yellow)**: High attention (model focuses here)\n",
    "- **Cool colors (blue)**: Low attention\n",
    "- Attention should focus on relevant anatomical regions\n",
    "- Example: When generating \"heart\", attention should focus on cardiac silhouette\n",
    "\n",
    "This provides interpretability and can help identify model failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(image_path, caption_words, attention_weights, save_path=None):\n",
    "    \"\"\"Visualize attention heatmaps over image for each word.\"\"\"\n",
    "    from scipy.ndimage import zoom\n",
    "    import matplotlib.gridspec as gridspec\n",
    "\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_array = np.array(img)\n",
    "\n",
    "    num_words = min(len(caption_words), 8)\n",
    "    fig = plt.figure(figsize=(16, 3 * ((num_words + 1) // 2)))\n",
    "    gs = gridspec.GridSpec((num_words + 1) // 2, 2, figure=fig)\n",
    "\n",
    "    for idx in range(num_words):\n",
    "        ax = fig.add_subplot(gs[idx // 2, idx % 2])\n",
    "\n",
    "        # Reshape 49-dim attention to 7x7, then resize to image size\n",
    "        attn = attention_weights[idx].cpu().numpy().reshape(7, 7)\n",
    "        zoom_factor = img_array.shape[0] / 7\n",
    "        attn_resized = zoom(attn, zoom_factor, order=1)\n",
    "\n",
    "        ax.imshow(img_array, alpha=0.7)\n",
    "        im = ax.imshow(attn_resized, cmap='jet', alpha=0.5)\n",
    "        ax.set_title(f'\"{caption_words[idx]}\"', fontsize=14, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.suptitle('Attention: Where Model Looks for Each Word', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Attention visualization function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 3 samples\n",
    "for idx in range(min(3, len(test_samples))):\n",
    "    sample = test_samples[idx]\n",
    "    caption_words = sample['generated'].split()\n",
    "\n",
    "    print(f\"\\nSample {idx+1}: {sample['generated']}\")\n",
    "\n",
    "    save_path = output_dir / f'attention_sample_{idx+1}.png'\n",
    "    visualize_attention(\n",
    "        sample['image_path'],\n",
    "        caption_words,\n",
    "        sample['attention_weights'],\n",
    "        save_path=save_path\n",
    "    )\n",
    "    print(f\"✓ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis\n",
    "\n",
    "Identify failure modes and improvement opportunities by analyzing:\n",
    "- **Best predictions**: What makes them good? (high BLEU-4)\n",
    "- **Worst predictions**: Common errors (low BLEU-4)\n",
    "- **Caption length correlation**: Does model struggle with long/short captions?\n",
    "- **BLEU distribution**: Are most captions good, or bimodal?\n",
    "\n",
    "### Common Error Patterns:\n",
    "1. **Generic captions**: Model produces vague descriptions (\"no acute findings\")\n",
    "2. **Missing details**: Misses specific abnormalities\n",
    "3. **Repetition**: Repeats words or phrases\n",
    "4. **Hallucination**: Describes findings not present in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-sample BLEU-4\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "per_sample_bleu = []\n",
    "for ref, hyp in zip(all_references, all_hypotheses):\n",
    "    bleu = sentence_bleu(ref, hyp, weights=(0, 0, 0, 1))\n",
    "    per_sample_bleu.append(bleu)\n",
    "\n",
    "print(f\"Per-sample BLEU-4 statistics:\")\n",
    "print(f\"  Mean: {np.mean(per_sample_bleu):.4f}\")\n",
    "print(f\"  Median: {np.median(per_sample_bleu):.4f}\")\n",
    "print(f\"  Std: {np.std(per_sample_bleu):.4f}\")\n",
    "print(f\"  Min: {np.min(per_sample_bleu):.4f}\")\n",
    "print(f\"  Max: {np.max(per_sample_bleu):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best predictions\n",
    "best_indices = np.argsort(per_sample_bleu)[-5:][::-1]\n",
    "print(\"Best Predictions (Highest BLEU-4):\")\n",
    "print(\"=\"*80)\n",
    "for rank, idx in enumerate(best_indices, 1):\n",
    "    print(f\"\\n[Rank {rank}] BLEU-4: {per_sample_bleu[idx]:.4f}\")\n",
    "    print(f\"  Reference:  {' '.join(all_references[idx][0])}\")\n",
    "    print(f\"  Generated:  {' '.join(all_hypotheses[idx])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst predictions\n",
    "worst_indices = np.argsort(per_sample_bleu)[:5]\n",
    "print(\"\\n\\nWorst Predictions (Lowest BLEU-4):\")\n",
    "print(\"=\"*80)\n",
    "for rank, idx in enumerate(worst_indices, 1):\n",
    "    print(f\"\\n[Rank {rank}] BLEU-4: {per_sample_bleu[idx]:.4f}\")\n",
    "    print(f\"  Reference:  {' '.join(all_references[idx][0])}\")\n",
    "    print(f\"  Generated:  {' '.join(all_hypotheses[idx])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(per_sample_bleu, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(np.mean(per_sample_bleu), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {np.mean(per_sample_bleu):.4f}')\n",
    "ax.axvline(np.median(per_sample_bleu), color='green', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {np.median(per_sample_bleu):.4f}')\n",
    "ax.set_xlabel('BLEU-4 Score', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Distribution of Per-Sample BLEU-4 Scores on Test Set', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_path = output_dir / 'bleu_distribution.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved to: {save_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Export\n",
    "\n",
    "Final summary of training results and next steps for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results\n",
    "test_results = {\n",
    "    'test_metrics': test_metrics,\n",
    "    'val_metrics': val_metrics,\n",
    "    'best_epoch': int(best_epoch['epoch']),\n",
    "    'total_epochs_trained': len(metrics_df),\n",
    "    'model_config': config,\n",
    "    'output_directory': str(output_dir)\n",
    "}\n",
    "\n",
    "with open(output_dir / 'test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(\"✓ Test results saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📊 Model: {variant_name}\")\n",
    "print(f\"📊 Best Epoch: {int(best_epoch['epoch'])}\")\n",
    "print(f\"📊 Training Time: {len(metrics_df)} epochs\")\n",
    "\n",
    "print(f\"\\n📈 Test Set Performance:\")\n",
    "print(f\"  • BLEU-4: {test_metrics['bleu_4']:.4f}\")\n",
    "print(f\"  • BLEU-3: {test_metrics['bleu_3']:.4f}\")\n",
    "print(f\"  • BLEU-2: {test_metrics['bleu_2']:.4f}\")\n",
    "print(f\"  • BLEU-1: {test_metrics['bleu_1']:.4f}\")\n",
    "print(f\"  • METEOR: {test_metrics['meteor']:.4f}\")\n",
    "print(f\"  • ROUGE-L: {test_metrics['rouge_l']:.4f}\")\n",
    "\n",
    "print(f\"\\n📁 Output Directory: {output_dir}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generated files\n",
    "print(\"\\n📄 Generated Files:\")\n",
    "generated_files = [\n",
    "    'checkpoints/best_model.pt',\n",
    "    'metrics.csv',\n",
    "    'training_manifest.json',\n",
    "    'test_results.json',\n",
    "    'training_curves.png',\n",
    "    'test_vs_val_metrics.png',\n",
    "    'bleu_distribution.png'\n",
    "]\n",
    "\n",
    "for fname in generated_files:\n",
    "    fpath = output_dir / fname\n",
    "    if fpath.exists():\n",
    "        print(f\"  ✓ {fname}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {fname} (not found)\")\n",
    "\n",
    "# Count attention visualizations\n",
    "attention_files = list((output_dir).glob('attention_sample_*.png'))\n",
    "print(f\"  ✓ {len(attention_files)} attention visualization(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Potential improvements to explore:\n",
    "\n",
    "1. **Fine-tune Encoder**: Unfreeze DenseNet layers for domain adaptation\n",
    "   ```python\n",
    "   model.encoder.unfreeze_layers(from_layer='denseblock3')\n",
    "   ```\n",
    "\n",
    "2. **Experiment with Beam Search**: Try different beam sizes (3, 5, 10) and length penalties\n",
    "\n",
    "3. **Alternative Attention**: Try Luong (multiplicative) attention for comparison\n",
    "\n",
    "4. **Ensemble Models**: Train multiple models and average predictions\n",
    "\n",
    "5. **Deploy for Inference**: Create inference script for real-time caption generation\n",
    "\n",
    "6. **Error Analysis**: Identify systematic failures and retrain on hard examples\n",
    "\n",
    "---\n",
    "\n",
    "**Training Complete!** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
